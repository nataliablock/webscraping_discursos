{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping dos discursos de Bolsonaro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste notebook você encontrará a programação em Python para coleta dos discursos, limpeza do texto e estruturação do banco de dados.\n",
    "\n",
    "#### Essas são as etapas da programação (serão enumeradas ao longo do script):\n",
    "\n",
    "1) Coleta dos códigos fonte das páginas que em que estão os links das transcrições dos discursos. Criação dos links da paginação e scraping dessas páginas.\n",
    "\n",
    "2) Limpeza dos códigos fonte das páginas e coleta de todos os links que levam à transcrição dos discursos.\n",
    "\n",
    "3) Scraping da transcrição dos discursos.\n",
    "\n",
    "4) Coleta da data dos discursos\n",
    "\n",
    "5) Coleta dos textos dos discursos\n",
    "\n",
    "6) Limpeza dos textos e estruturação em banco de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonte dos dados:\n",
    "Site do Planalto: https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregando os pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulação de dados\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import unidecode\n",
    "import requests\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Web scraping e processamento\n",
    "\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping \n",
    "Última coleta realizada no dia 2 de setembro de 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Coleta dos códigos fonte das páginas que em que estão os links das transcrições dos discursos. Criação dos links da paginação e scraping dessas páginas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primeiro preciso fazer um loop para abrir os links da paginação\n",
    "# para em seguida coletar os links para os discursos dentro de cada \n",
    "#página.\n",
    "\n",
    "#Primeiro crio uma lista com os números id das páginas\n",
    "pgs = list(range(0,270,30))\n",
    "\n",
    "#Agora crio os loops para salvar as urls criadas em uma lista\n",
    "lista_urls = []\n",
    "for i in pgs:\n",
    "    url_completa = 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=' + str(i)\n",
    "    lista_urls.append(url_completa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=0',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=30',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=60',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=90',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=120',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=150',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=180',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=210',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=240']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualizando a lista de urls\n",
    "lista_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para fazer o scraping do código fonte das páginas (crawling da\n",
    "#paginação). \n",
    "\n",
    "\n",
    "#Criando um contador para as paginas. São 9 páginas\n",
    "contador = 9\n",
    "\n",
    "#Criando a função\n",
    "def scraping_urls(urls):\n",
    "    \n",
    "    # Define o driver. Estou usando o meu browser padrão que é o Safari\n",
    "    driver = webdriver.Safari()\n",
    "    \n",
    "    # Lista para o resultado\n",
    "    soup_list = []\n",
    " \n",
    "    \n",
    "    # Contador\n",
    "    count = 0\n",
    "    \n",
    "    # Loop pelas urls\n",
    "    for i in urls:\n",
    "        if count < contador:\n",
    "            driver.get(i)\n",
    "            driver.refresh()\n",
    "            time.sleep(5)\n",
    "            soup_list.append(BeautifulSoup(driver.page_source, 'html.parser'))\n",
    "        count += 1\n",
    "    driver.close()\n",
    "    return soup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazendo o scraping das páginas\n",
    "fonte= scraping_urls(lista_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Limpeza dos códigos fonte das páginas e coleta de todos os links que levam à transcrição dos discursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coletando apenas as tags em que estão os links\n",
    "tag_links=[]\n",
    "for f in fonte:\n",
    "    link=f.find_all('a', class_=\"summary url\")\n",
    "    tag_links.append(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalmente coletando os links dentro das tags\n",
    "links=[]\n",
    "for i in tag_links:\n",
    "    for j in i:\n",
    "        link=re.search(r'<a class=\"summary url\" href=\\\"(.*?)\" title=\"Document\">', str(j)).group(1)\n",
    "        links.append(link)                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Scraping da transcrição dos discursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a função para fazer o scraping do texto dos discursos dos \n",
    "#links coletados acima\n",
    "\n",
    "def extrai_discurso(urls):\n",
    "    driver = webdriver.Safari()\n",
    "    doc_source = []\n",
    "    for i in urls:\n",
    "        driver.get(i)\n",
    "        time.sleep(5)\n",
    "        doc_source.append(BeautifulSoup(driver.page_source, 'html.parser'))\n",
    "    driver.close()\n",
    "    return doc_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a função à lista de links\n",
    "#Encerrar a seção do webdriver anterior caso ela ainda esteja aberta!\n",
    "discursos = extrai_discurso(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Coleta da data dos discursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Farei duas extrações da lista de links: uma para coletar as datas dos\n",
    "#discursos e outra para coletar o texto em si. Vou começar coletando\n",
    "#as datas.\n",
    "\n",
    "#função para retirar data da tag span class=\"documentPublished\"\n",
    "def coleta_data(soup_object):\n",
    "    data_list = []\n",
    "    for s in soup_object:\n",
    "        data= s.find_all('span', class_= 'documentPublished')\n",
    "        data_list.append(data)\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop para extração da data. Salvando as datas em lista\n",
    "lista_data=[]\n",
    "for i in coleta_data(discursos):\n",
    "    lista_data.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraindo resultado da coleta\n",
    "datas= coleta_data(discursos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop para coletar apenas a data que aparece entre as tags span\n",
    "#de class=value e para retirar o horário\n",
    "data_limpa=[]\n",
    "for c in datas:\n",
    "    cleaner= re.search(r'<span class=\"value\"\\>(.*?)</span>',str(c)).group(1)\n",
    "    data_limpa.append(cleaner)\n",
    "\n",
    "data_s_hora=[]\n",
    "for d in data_limpa:\n",
    "    sem_hora=d.split(' ',1)[0]\n",
    "    data_s_hora.append(sem_hora)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Coleta dos textos dos discursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agora seguirei para a extração dos textos dos discursos\n",
    "\n",
    "# Função para extrair o texto dos discursos dos links. Uso o beautiful\n",
    "#soup para encontrar as tags html de parágrafo ('p') e salvo os textos\n",
    "#em lista\n",
    "def extrai_texto(soup_object):\n",
    "    text_list = []\n",
    "    for s in soup_object:\n",
    "        text = s.find_all('p')\n",
    "        text_list.append(text)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop para extração do texto. Salvo os textos em lista.\n",
    "discurso_lista = []\n",
    "for i in extrai_texto(discursos):\n",
    "    discurso_lista.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai os discursos\n",
    "primeiros_discursos = extrai_texto(discursos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificando quantos discursos foram coletados\n",
    "len(primeiros_discursos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dando uma olhada nos textos. Pegando apenas o primeiro da lista\n",
    "primeiros_discursos[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza do texto e estruturação do banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover tags html\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicando a função para remover tags\n",
    "strings=[]\n",
    "for d in primeiros_discursos:\n",
    "    cleaned=remove_html_tags(str(d))\n",
    "    strings.append(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retirando \\xa0 e colocando tudo em letras minúsculas.\n",
    "limpos=[ ]\n",
    "for i in strings:\n",
    "    tira_xa=i.replace('\\xa0', ' ').lower()\n",
    "    limpos.append(tira_xa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Existem casos no texto em que não existe espaço entre pontuação e \n",
    "#palavras. Desta forma, precisarei incluir espaços entre pontuações\n",
    "#e palavras antes de retirar a pontuação, assim evito que palavras \n",
    "#sejam concatenadas. Logo depois retirarei espaços em excesso.\n",
    "\n",
    "#Incluindo espaços\n",
    "\n",
    "limpos_espaco=[]\n",
    "for i in limpos:\n",
    "    esp=re.sub('([.,!?()])', r' \\1 ', i)\n",
    "    limpos_espaco.append(esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando função para remover pontuação\n",
    "def remove_pontuacao(valor):\n",
    "    result = \"\"\n",
    "    for c in valor:\n",
    "        if c not in string.punctuation:\n",
    "            result += c\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicando função para retirar pontuação na lista limpos\n",
    "mais_limpos=[]\n",
    "for i in limpos_espaco:\n",
    "    tira_pontuacao=remove_pontuacao(i)\n",
    "    mais_limpos.append(tira_pontuacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retirando excesso de espaços em branco\n",
    "s_espaco=[]\n",
    "for i in mais_limpos:\n",
    "    limpa_espaco=i.replace(\"    \", \" \").replace(\"   \", \" \").replace(\"  \", \" \")\n",
    "    s_espaco.append(limpa_espaco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retirando acentos\n",
    "texto_final=[]\n",
    "for i in s_espaco:\n",
    "    s_acento=unidecode.unidecode(i)\n",
    "    texto_final.append(s_acento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatando as datas da lista data_s_hora (criada na seção anterior durante\n",
    "#a coleta de string para date e salvando em um objeto do tipo pandas\n",
    "data_final=pd.to_datetime(data_s_hora, format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando objeto pandas apenas com os anos\n",
    "anos=pd.DatetimeIndex(data_final).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalmente criando um data frame com data, ano, link do discurso e o\n",
    "#texto do discurso\n",
    "\n",
    "bolsonaro=pd.DataFrame([data_final, anos, links, texto_final]).T\n",
    "bolsonaro.columns=['data', 'ano', 'link', 'transcricao']\n",
    "bolsonaro.insert(0,'presidente', 'bolsonaro')\n",
    "bolsonaro.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvando o banco como csv:\n",
    "bolsonaro.to_csv('discursos_bolsonaro.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
